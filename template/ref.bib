
@article{boukhers_coin_2022,
	title = {{COIN}: Counterfactual Image Generation for Visual Question Answering Interpretation},
	volume = {22},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/6/2245},
	doi = {10.3390/s22062245},
	shorttitle = {{COIN}},
	abstract = {Due to the significant advancement of Natural Language Processing and Computer Vision-based models, Visual Question Answering ({VQA}) systems are becoming more intelligent and advanced. However, they are still error-prone when dealing with relatively complex questions. Therefore, it is important to understand the behaviour of the {VQA} models before adopting their results. In this paper, we introduce an interpretability approach for {VQA} models by generating counterfactual images. Specifically, the generated image is supposed to have the minimal possible change to the original image and leads the {VQA} model to give a different answer. In addition, our approach ensures that the generated image is realistic. Since quantitative metrics cannot be employed to evaluate the interpretability of the model, we carried out a user study to assess different aspects of our approach. In addition to interpreting the result of {VQA} models on single images, the obtained results and the discussion provides an extensive explanation of {VQA} models’ behaviour.},
	pages = {2245},
	number = {6},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Boukhers, Zeyd and Hartmann, Timo and Jürjens, Jan},
	urldate = {2025-04-28},
	date = {2022-03-14},
	langid = {english},
	file = {Full Text:C\:\\Users\\beheerder\\Zotero\\storage\\3EFMU85A\\Boukhers et al. - 2022 - COIN Counterfactual Image Generation for Visual Question Answering Interpretation.pdf:application/pdf},
}

@article{hessel_clipscore_2021,
	title = {{CLIPScore}: A Reference-free Evaluation Metric for Image Captioning},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2104.08718},
	doi = {10.48550/ARXIV.2104.08718},
	shorttitle = {{CLIPScore}},
	abstract = {Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality. In this paper, we report the surprising empirical finding that {CLIP} (Radford et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, {CLIPScore}, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like {CIDEr} and {SPICE}. Information gain experiments demonstrate that {CLIPScore}, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that emphasize text-text similarities. Thus, we also present a reference-augmented version, {RefCLIPScore}, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where {CLIPScore} performs well (clip-art images, alt-text rating), but also where it is relatively weaker in comparison to reference-based metrics, e.g., news captions that require richer contextual knowledge.},
	author = {Hessel, Jack and Holtzman, Ari and Forbes, Maxwell and Bras, Ronan Le and Choi, Yejin},
	urldate = {2025-04-28},
	date = {2021},
	note = {Publisher: {arXiv}
Version Number: 3},
	keywords = {Computation and Language (cs.{CL}), Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
	file = {PDF:C\:\\Users\\beheerder\\Zotero\\storage\\7XSE8T2Y\\Hessel et al. - 2021 - CLIPScore A Reference-free Evaluation Metric for Image Captioning.pdf:application/pdf},
}

@article{heusel_gans_2017,
	title = {{GANs} Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1706.08500},
	doi = {10.48550/ARXIV.1706.08500},
	abstract = {Generative Adversarial Networks ({GANs}) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of {GAN} training has still not been proved. We propose a two time-scale update rule ({TTUR}) for training {GANs} with stochastic gradient descent on arbitrary {GAN} loss functions. {TTUR} has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the {TTUR} converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of {GANs} at image generation, we introduce the "Fréchet Inception Distance" ({FID}) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, {TTUR} improves learning for {DCGANs} and Improved Wasserstein {GANs} ({WGAN}-{GP}) outperforming conventional {GAN} training on {CelebA}, {CIFAR}-10, {SVHN}, {LSUN} Bedrooms, and the One Billion Word Benchmark.},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	urldate = {2025-04-28},
	date = {2017},
	note = {Publisher: {arXiv}
Version Number: 6},
	keywords = {{FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Machine Learning (stat.{ML})},
	file = {PDF:C\:\\Users\\beheerder\\Zotero\\storage\\B24MY7EU\\Heusel et al. - 2017 - GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.pdf:application/pdf},
}

@misc{hu_lora_2021,
	title = {{LoRA}: Low-Rank Adaptation of Large Language Models},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	shorttitle = {{LoRA}},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using {GPT}-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or {LoRA}, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to {GPT}-3 175B fine-tuned with Adam, {LoRA} can reduce the number of trainable parameters by 10,000 times and the {GPU} memory requirement by 3 times. {LoRA} performs on-par or better than fine-tuning in model quality on {RoBERTa}, {DeBERTa}, {GPT}-2, and {GPT}-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of {LoRA}. We release a package that facilitates the integration of {LoRA} with {PyTorch} models and provide our implementations and model checkpoints for {RoBERTa}, {DeBERTa}, and {GPT}-2 at https://github.com/microsoft/{LoRA}.},
	number = {{arXiv}:2106.09685},
	publisher = {{arXiv}},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	urldate = {2025-04-28},
	date = {2021-10-16},
	eprinttype = {arxiv},
	eprint = {2106.09685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\beheerder\\Zotero\\storage\\4QMXN4V6\\Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\beheerder\\Zotero\\storage\\T5ZL3SH9\\2106.html:text/html},
}

@misc{karras_progressive_2018,
	title = {Progressive Growing of {GANs} for Improved Quality, Stability, and Variation},
	url = {http://arxiv.org/abs/1710.10196},
	doi = {10.48550/arXiv.1710.10196},
	abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., {CelebA} images at 1024{\textasciicircum}2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised {CIFAR}10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating {GAN} results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the {CelebA} dataset.},
	number = {{arXiv}:1710.10196},
	publisher = {{arXiv}},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	urldate = {2025-04-28},
	date = {2018-02-26},
	eprinttype = {arxiv},
	eprint = {1710.10196 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\beheerder\\Zotero\\storage\\68E7D2KX\\Karras et al. - 2018 - Progressive Growing of GANs for Improved Quality, Stability, and Variation.pdf:application/pdf;Snapshot:C\:\\Users\\beheerder\\Zotero\\storage\\RI4Q8YKK\\1710.html:text/html},
}

@misc{melistas_benchmarking_2025,
	title = {Benchmarking Counterfactual Image Generation},
	url = {http://arxiv.org/abs/2403.20287},
	doi = {10.48550/arXiv.2403.20287},
	abstract = {Generative {AI} has revolutionised visual content editing, empowering users to effortlessly modify images and videos. However, not all edits are equal. To perform realistic edits in domains such as natural image or medical imaging, modifications must respect causal relationships inherent to the data generation process. Such image editing falls into the counterfactual image generation regime. Evaluating counterfactual image generation is substantially complex: not only it lacks observable ground truths, but also requires adherence to causal constraints. Although several counterfactual image generation methods and evaluation metrics exist, a comprehensive comparison within a unified setting is lacking. We present a comparison framework to thoroughly benchmark counterfactual image generation methods. We integrate all models that have been used for the task at hand and expand them to novel datasets and causal graphs, demonstrating the superiority of Hierarchical {VAEs} across most datasets and metrics. Our framework is implemented in a user-friendly Python package that can be extended to incorporate additional {SCMs}, causal methods, generative models, and datasets for the community to build on. Code: https://github.com/gulnazaki/counterfactual-benchmark.},
	number = {{arXiv}:2403.20287},
	publisher = {{arXiv}},
	author = {Melistas, Thomas and Spyrou, Nikos and Gkouti, Nefeli and Sanchez, Pedro and Vlontzos, Athanasios and Panagakis, Yannis and Papanastasiou, Giorgos and Tsaftaris, Sotirios A.},
	urldate = {2025-04-28},
	date = {2025-01-13},
	eprinttype = {arxiv},
	eprint = {2403.20287 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\beheerder\\Zotero\\storage\\SU3QFMC6\\Melistas et al. - 2025 - Benchmarking Counterfactual Image Generation.pdf:application/pdf;Snapshot:C\:\\Users\\beheerder\\Zotero\\storage\\THDGSZBS\\2403.html:text/html},
}

@misc{rombach_high-resolution_2022,
	title = {High-Resolution Image Synthesis with Latent Diffusion Models},
	url = {http://arxiv.org/abs/2112.10752},
	doi = {10.48550/arXiv.2112.10752},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models ({DMs}) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful {DMs} often consumes hundreds of {GPU} days and inference is expensive due to sequential evaluations. To enable {DM} training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models ({LDMs}) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based {DMs}. Code is available at https://github.com/{CompVis}/latent-diffusion .},
	number = {{arXiv}:2112.10752},
	publisher = {{arXiv}},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	urldate = {2025-04-28},
	date = {2022-04-13},
	eprinttype = {arxiv},
	eprint = {2112.10752 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\beheerder\\Zotero\\storage\\HYSJBLR9\\Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffusion Models.pdf:application/pdf;Snapshot:C\:\\Users\\beheerder\\Zotero\\storage\\JFXJLJ5D\\2112.html:text/html},
}

@inproceedings{zhang_unreasonable_2018,
	title = {The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
	url = {https://ieeexplore.ieee.org/document/8578166},
	doi = {10.1109/CVPR.2018.00068},
	abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as {PSNR} and {SSIM}, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the {VGG} network trained on {ImageNet} classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to {ImageNet}-trained {VGG} features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
	eventtitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	pages = {586--595},
	booktitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
	urldate = {2025-04-28},
	date = {2018-06},
	note = {{ISSN}: 2575-7075},
	keywords = {Computer architecture, Distortion, Measurement, Network architecture, Task analysis, Training, Visualization},
	file = {Snapshot:C\:\\Users\\beheerder\\Zotero\\storage\\WJUDQ6BQ\\8578166.html:text/html;Submitted Version:C\:\\Users\\beheerder\\Zotero\\storage\\RV4ARM44\\Zhang et al. - 2018 - The Unreasonable Effectiveness of Deep Features as a Perceptual Metric.pdf:application/pdf},
}

@misc{xie_visual_2019,
	title = {Visual Entailment Task for Visually-Grounded Language Learning},
	url = {http://arxiv.org/abs/1811.10582},
	doi = {10.48550/arXiv.1811.10582},
	abstract = {We introduce a new inference task - Visual Entailment ({VE}) - which differs from traditional Textual Entailment ({TE}) tasks whereby a premise is defined by an image, rather than a natural language sentence as in {TE} tasks. A novel dataset {SNLI}-{VE} (publicly available at https://github.com/necla-ml/{SNLI}-{VE}) is proposed for {VE} tasks based on the Stanford Natural Language Inference corpus and Flickr30k. We introduce a differentiable architecture called the Explainable Visual Entailment model ({EVE}) to tackle the {VE} problem. {EVE} and several other state-of-the-art visual question answering ({VQA}) based models are evaluated on the {SNLI}-{VE} dataset, facilitating grounded language understanding and providing insights on how modern {VQA} based models perform.},
	number = {{arXiv}:1811.10582},
	publisher = {{arXiv}},
	author = {Xie, Ning and Lai, Farley and Doran, Derek and Kadav, Asim},
	urldate = {2025-05-08},
	date = {2019-01-21},
	eprinttype = {arxiv},
	eprint = {1811.10582 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\beheerder\\Zotero\\storage\\5A2IBC99\\Xie et al. - 2019 - Visual Entailment Task for Visually-Grounded Language Learning.pdf:application/pdf;Snapshot:C\:\\Users\\beheerder\\Zotero\\storage\\7XN53RUS\\1811.html:text/html},
}
